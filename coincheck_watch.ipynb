{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bita46a4a11000847309cdd4e52c21c0e11",
   "display_name": "Python 3.7.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "with open(\"C:\\\\Users\\\\thang\\\\OneDrive\\\\Documents\\\\binance_cres\\\\api_key.txt\") as api_file:\n",
    "    api_key = api_file.readline()\n",
    "with open(\"C:\\\\Users\\\\thang\\\\OneDrive\\\\Documents\\\\binance_cres\\\\sec_key.txt\") as api_file:\n",
    "    api_secret = api_file.readline()\n",
    "client = Client(api_key, api_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Raw data\n",
    "# agg_trades = client.aggregate_trade_iter(symbol='ETCUSDT', start_str='60 minutes ago UTC')\n",
    "\n",
    "# # iterate over the trade iterator\n",
    "# df_dict = {'a':[], 'p':[], 'q':[], 'f':[], 'l':[],'T':[], 'm':[],'M':[]}\n",
    "# for trade in agg_trades:\n",
    "#     # prices.append(trade[\"p\"])\n",
    "#     df_dict['a'].append(trade[\"a\"])\n",
    "#     df_dict['p'].append(trade[\"p\"])\n",
    "#     df_dict['q'].append(trade[\"q\"])\n",
    "#     df_dict['f'].append(trade[\"f\"])\n",
    "#     df_dict['l'].append(trade[\"l\"])\n",
    "#     df_dict['T'].append(trade[\"T\"])\n",
    "#     df_dict['m'].append(trade[\"m\"])\n",
    "#     df_dict['M'].append(trade[\"M\"])\n",
    "#     print(trade)\n",
    "\n",
    "# df = pd.DataFrame.from_dict(df_dict)\n",
    "# interested_df = df[['p','q','T']]  # get price as p, quantity as q, Time as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = client.get_klines(symbol='ETCUSDT', interval=Client.KLINE_INTERVAL_30MINUTE)\n",
    "cols = ['open_time',  # Open time\n",
    "            'open',  # Open\n",
    "            'high',  # High\n",
    "            'low',  # Low\n",
    "            'close',  # Close\n",
    "            'vol',  # Volume\n",
    "            'close_time',  # Close time\n",
    "            'quote_ass_vol',  # Quote asset volume\n",
    "            'no_trade',  # Number of trades\n",
    "            'base_ass_vol',  # Taker buy base asset volume\n",
    "            'qoute_ass_vol',  # Taker buy quote asset volume\n",
    "            'ignore']   # Can be ignored\n",
    "df = pd.DataFrame(avg_data, \n",
    "    columns = cols)\n",
    "\n",
    "important_features_df = df[['close','close_time','vol']]  # get important features\n",
    "important_features_df.close_time = important_features_df.close_time.apply(lambda x: x/1000)  # convert milisecond to second\n",
    "# important_features_df.close_time = important_features_df.close_time.apply(lambda x: datetime.fromtimestamp(x/1000))  # convert to normal time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = important_features_df.pop('close_time')  # extract date_time as seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_df.describe()  # inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time data to vector as it is better for model\n",
    "\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "important_features_df['Day sin'] = np.sin(date_time * (2 * np.pi / day))\n",
    "important_features_df['Day cos'] = np.cos(date_time * (2 * np.pi / day))\n",
    "important_features_df['Year sin'] = np.sin(date_time * (2 * np.pi / year))\n",
    "important_features_df['Year cos'] = np.cos(date_time * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features_df.close = important_features_df.close.astype(float)  # make sure close is float\n",
    "important_features_df.vol = important_features_df.vol.astype(float)  # make sure vol is float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(important_features_df['Day sin'])[:25])\n",
    "plt.plot(np.array(important_features_df['Day cos'])[:25])\n",
    "plt.xlabel('Time [h]')\n",
    "plt.title('Time of day signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data splitting\n",
    "# column_indices = {name: i for i, name in enumerate(important_features_df.columns)}\n",
    "\n",
    "n = len(important_features_df)\n",
    "train_df = important_features_df[0:int(n*0.7)]  # 70% train data\n",
    "val_df = important_features_df[int(n*0.7):int(n*0.9)]  # 20% val data\n",
    "test_df = important_features_df[int(n*0.9):]  # 10% test data\n",
    "\n",
    "num_features = important_features_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "# make sure train data as main distribution\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_predictors = 24  # number of historical data as predictor\n",
    "time_shift = 24  # number of timestamp to target\n",
    "label_width = 1  # number of predicting timestamp\n",
    "\n",
    "class WindowGenerator():\n",
    "    \"\"\"\n",
    "    This class make indices base on inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "                label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in\n",
    "                                enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)  # make slice for input\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]  # get indexes of time features\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)  # make slice for label from start\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]  # get indexes of time features\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def window_maker(self, data_matrix):\n",
    "        \"\"\"\n",
    "        Make window function\n",
    "        This function extract timestamp as features.\n",
    "        It also extract timestamp as labels\n",
    "        Args:\n",
    "\n",
    "        Output:\n",
    "            inputs: index of features in input matrix\n",
    "            labels: index of label in input matrix\n",
    "        \"\"\"\n",
    "        inputs = data_matrix[:, self.input_slice, :]  # extract inputs data using indexes\n",
    "        labels = data_matrix[:, self.labels_slice, :]  # extract labels data using indexes\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack([labels[:, :, self.column_indices[name]] for name in self.label_columns],axis=-1)\n",
    "        \n",
    "        \n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def plot(self, model=None, plot_col='close', max_subplots=3):\n",
    "        \"\"\"\n",
    "        Plot function\n",
    "        \"\"\"\n",
    "        inputs, labels = self.example\n",
    "        print(\"len of input\", len(inputs))\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index], edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index], marker='X', edgecolors='k', label='Predictions', c='#ff7f0e', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel('Time [h]')\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        \"\"\"\n",
    "        convert dataframe to matrix of shape (batch, time features, static features)\n",
    "        Input:\n",
    "            dataframe\n",
    "        Output:\n",
    "            tensordata\n",
    "        \"\"\"\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        \n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)  # hardcore batchsize, need to change later\n",
    "\n",
    "        ds = ds.map(self.window_maker)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "            # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = WindowGenerator(input_width=6, label_width=1, shift=1,\n",
    "                     label_columns=['close'])\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batch\n",
    "example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
    "                           np.array(train_df[100:100+w2.total_window_size]),\n",
    "                           np.array(train_df[200:200+w2.total_window_size])])\n",
    "\n",
    "\n",
    "example_inputs, example_labels = w2.window_maker(example_window)  # divide into input and label by batch\n",
    "\n",
    "print('All shapes are: (batch, time, features)')\n",
    "print(f'Window shape: {example_window.shape}')\n",
    "print(f'Inputs shape: {example_inputs.shape}')\n",
    "print(f'labels shape: {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[:w2.total_window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.plot(plot_col='close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "single_step_window = WindowGenerator(\n",
    "    input_width=24, label_width=1, shift=1, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=['close'])\n",
    "single_step_window\n",
    "\n",
    "# model\n",
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, label_index=None):\n",
    "        super().__init__()\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            return inputs\n",
    "        result = inputs[:, :, self.label_index]\n",
    "        return result[:, :, tf.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    input_width=24, label_width=24, shift=1,\n",
    "    label_columns=['close'])\n",
    "\n",
    "wide_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', wide_window.example[0].shape)\n",
    "print('Output shape:', baseline(wide_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(baseline, max_subplots=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main model\n",
    "linear = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    # tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1),\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "    # tf.keras.layers.Reshape([1, -1]),\n",
    "])\n",
    "\n",
    "# training setting\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(linear, single_step_window)\n",
    "# linear.summary()\n",
    "val_performance['Linear'] = linear.evaluate(single_step_window.val)\n",
    "performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x = range(len(train_df.columns)),\n",
    "        height=linear.layers[0].kernel[:,0].numpy())\n",
    "axis = plt.gca()\n",
    "axis.set_xticks(range(len(train_df.columns)))\n",
    "_ = axis.set_xticklabels(train_df.columns, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input shape:', wide_window.example[0].shape)\n",
    "print('Output shape:', linear(wide_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}